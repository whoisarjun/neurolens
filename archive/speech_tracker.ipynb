{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38507c7",
   "metadata": {},
   "source": [
    "# Dementia Speech Trend Tracker (Hackathon Edition)\n",
    "So this is the quick notebook we smashed together during the hackathon.\n",
    "It yanks features from Whisper text and tries to guess MMSE, MoCA, and CDR.\n",
    "Charts and stuff show how the patient's speech is drifting over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d120fd",
   "metadata": {},
   "source": [
    "## Install deps real quickGrab everything we'll need. Sorry if this takes a sec."
   ]
  },
  {
   "cell_type": "code",
   "id": "186a2107",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T12:57:18.391576Z",
     "start_time": "2025-07-09T12:57:09.443956Z"
    }
   },
   "source": [
    "# quick install, fingers crossed\n",
    "!pip install numpy==1.24.4 nltk spacy benepar textstat gensim scikit-learn matplotlib tqdm sentence-transformers --quiet\n",
    "!python3 -m spacy download en_core_web_sm --quiet\n",
    "!python3 -m benepar.download benepar_en3 --quiet "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2mâœ” Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n",
      "/Users/arjunpalakkal/Documents/[01] Personal/[01] Coding Projects/[02] Github Stuff/[01] Graphite/.venv/bin/python3: No module named benepar.download\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "38744252",
   "metadata": {},
   "source": [
    "## Imports and setupAll the usual libraries. It's a bit messy but works."
   ]
  },
  {
   "cell_type": "code",
   "id": "866720b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T12:57:02.094685Z",
     "start_time": "2025-07-09T12:57:02.027377Z"
    }
   },
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "import benepar\n",
    "import textstat\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "if not nlp.has_pipe('benepar'):\n",
    "    nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "baseline = {}  # patient -> feature dict\n",
    "history = {}   # patient -> list of daily records\n"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (/Users/arjunpalakkal/Documents/[01] Personal/[01] Coding Projects/[02] Github Stuff/[01] Graphite/.venv/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mbenepar\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtextstat\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[1;32m     14\u001B[0m nlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m nlp\u001B[38;5;241m.\u001B[39mhas_pipe(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbenepar\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/Documents/[01] Personal/[01] Coding Projects/[02] Github Stuff/[01] Graphite/.venv/lib/python3.10/site-packages/sentence_transformers/__init__.py:14\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     10\u001B[0m     export_dynamic_quantized_onnx_model,\n\u001B[1;32m     11\u001B[0m     export_optimized_onnx_model,\n\u001B[1;32m     12\u001B[0m     export_static_quantized_openvino_model,\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     15\u001B[0m     CrossEncoder,\n\u001B[1;32m     16\u001B[0m     CrossEncoderModelCardData,\n\u001B[1;32m     17\u001B[0m     CrossEncoderTrainer,\n\u001B[1;32m     18\u001B[0m     CrossEncoderTrainingArguments,\n\u001B[1;32m     19\u001B[0m )\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mLoggingHandler\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "File \u001B[0;32m~/Documents/[01] Personal/[01] Coding Projects/[02] Github Stuff/[01] Graphite/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m__future__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_card\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderModelCardData\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderTrainer\n",
      "File \u001B[0;32m~/Documents/[01] Personal/[01] Coding Projects/[02] Github Stuff/[01] Graphite/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m nn\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautonotebook\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m trange\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     20\u001B[0m     AutoConfig,\n\u001B[1;32m     21\u001B[0m     AutoModelForSequenceClassification,\n\u001B[1;32m     22\u001B[0m     AutoTokenizer,\n\u001B[1;32m     23\u001B[0m     PretrainedConfig,\n\u001B[1;32m     24\u001B[0m     PreTrainedModel,\n\u001B[1;32m     25\u001B[0m     PreTrainedTokenizer,\n\u001B[1;32m     26\u001B[0m )\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PushToHubMixin\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m deprecated\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'PreTrainedModel' from 'transformers' (/Users/arjunpalakkal/Documents/[01] Personal/[01] Coding Projects/[02] Github Stuff/[01] Graphite/.venv/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "a3ce362c",
   "metadata": {},
   "source": [
    "## compute_features (sorry for the long function)Pulls a bunch of things out of a transcript. Works, but could be prettier."
   ]
  },
  {
   "cell_type": "code",
   "id": "197d39f1",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def compute_features(\n",
    "    transcript_text: str,\n",
    "    duration_sec: float,\n",
    "    segments: Optional[List[Dict[str, Any]]] = None,\n",
    "    baseline_embedding: Optional[np.ndarray] = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute linguistic features from a transcript.\"\"\"\n",
    "\n",
    "    if segments is None:\n",
    "        segments = []\n",
    "\n",
    "    # --- basic tokenization ---\n",
    "    try:\n",
    "        words = word_tokenize(transcript_text.lower())\n",
    "    except Exception:\n",
    "        words = []\n",
    "    try:\n",
    "        sentences = sent_tokenize(transcript_text)\n",
    "    except Exception:\n",
    "        sentences = []\n",
    "\n",
    "    total_words = len(words)\n",
    "    num_sentences = len(sentences)\n",
    "    unique_words = len(set(words))\n",
    "\n",
    "    # Words spoken per second\n",
    "    speech_speed = total_words / duration_sec if duration_sec > 0 else 0.0\n",
    "\n",
    "    # --- pause statistics ---\n",
    "    marker_pauses = len(re.findall(r\"\\.\\.\\.|<pause>\", transcript_text))\n",
    "    gap_durations = []\n",
    "    for i in range(1, len(segments)):\n",
    "        try:\n",
    "            prev_end = float(segments[i - 1][\"end\"])\n",
    "            start = float(segments[i][\"start\"])\n",
    "            gap = start - prev_end\n",
    "        except Exception:\n",
    "            continue\n",
    "        if gap > 0.3:\n",
    "            gap_durations.append(gap)\n",
    "    num_pauses = marker_pauses + len(gap_durations)\n",
    "    pause_mean = float(np.mean(gap_durations)) if gap_durations else 0.0\n",
    "    pause_var = float(np.var(gap_durations)) if gap_durations else 0.0\n",
    "\n",
    "    # --- vocabulary statistics ---\n",
    "    vocab_richness = unique_words / total_words if total_words > 0 else 0.0\n",
    "    # filler_count = sum(1 for word in words if word in FILLER_WORDS)\n",
    "    # filler_word_rate = filler_count / total_words if total_words > 0 else 0.0\n",
    "    if total_words > 0:\n",
    "        lexical_diversity = textstat.lexicon_count(transcript_text) / total_words\n",
    "    else:\n",
    "        lexical_diversity = 0.0\n",
    "    avg_sentence_length = total_words / num_sentences if num_sentences > 0 else 0.0\n",
    "\n",
    "    # --- syntactic features ---\n",
    "    doc = nlp(transcript_text) if transcript_text.strip() else None\n",
    "    #avg_parse_depth = get_avg_parse_depth(doc) if doc else 0.0 \n",
    "    dep_lengths = []\n",
    "    if doc:\n",
    "        for tok in doc:\n",
    "            if tok.dep_ != \"ROOT\":\n",
    "                dep_lengths.append(abs(tok.i - tok.head.i))\n",
    "    avg_dependency_length = float(np.mean(dep_lengths)) if dep_lengths else 0.0\n",
    "\n",
    "    # --- articulation ---\n",
    "    syllable_count = textstat.syllable_count(transcript_text)\n",
    "    speech_articulation_rate = syllable_count / duration_sec if duration_sec > 0 else 0.0\n",
    "\n",
    "    # --- discourse coherence ---\n",
    "    #coherence_score = compute_coherence_score(transcript_text) \n",
    "\n",
    "    # --- repetition ---\n",
    "    word_counts = Counter(words)\n",
    "    repeated_words = 0\n",
    "    for w, c in word_counts.items():\n",
    "        if c > 1:\n",
    "            repeated_words += 1\n",
    "    repetition_rate = repeated_words / total_words if total_words > 0 else 0.0\n",
    "\n",
    "    # --- pronoun vs noun usage ---\n",
    "    pronoun_count = 0\n",
    "    noun_count = 0\n",
    "    if doc:\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PRON\":\n",
    "                pronoun_count += 1\n",
    "            elif token.pos_ == \"NOUN\":\n",
    "                noun_count += 1\n",
    "    pronoun_noun_ratio = pronoun_count / noun_count if noun_count > 0 else 0.0\n",
    "\n",
    "    # --- verb tense ratios ---\n",
    "    verbs = []\n",
    "    if doc:\n",
    "        for token in doc:\n",
    "            if token.pos_ in {\"VERB\", \"AUX\"} or token.tag_ == \"MD\":\n",
    "                verbs.append(token)\n",
    "    verb_total = len(verbs)\n",
    "\n",
    "    present_count = 0\n",
    "    past_count = 0\n",
    "    future_count = 0\n",
    "    for token in verbs:\n",
    "        if \"Tense=Pres\" in token.morph or token.tag_ in {\"VBP\", \"VBZ\", \"VBG\"}:\n",
    "            present_count += 1\n",
    "        if \"Tense=Past\" in token.morph or token.tag_ in {\"VBD\", \"VBN\"}:\n",
    "            past_count += 1\n",
    "        if token.tag_ == \"MD\" or \"Tense=Fut\" in token.morph:\n",
    "            future_count += 1\n",
    "    tense_ratio_present = present_count / verb_total if verb_total > 0 else 0.0\n",
    "    tense_ratio_past = past_count / verb_total if verb_total > 0 else 0.0\n",
    "    tense_ratio_future = future_count / verb_total if verb_total > 0 else 0.0\n",
    "\n",
    "    # --- embedding similarity to baseline ---\n",
    "    semantic_similarity_drift = 0.0\n",
    "    if baseline_embedding is not None:\n",
    "        try:\n",
    "            emb = sentence_model.encode(transcript_text)\n",
    "            dot = float(np.dot(emb, baseline_embedding))\n",
    "            denom = np.linalg.norm(emb) * np.linalg.norm(baseline_embedding)\n",
    "            similarity = dot / denom\n",
    "            semantic_similarity_drift = 1.0 - similarity\n",
    "        except Exception:\n",
    "            semantic_similarity_drift = 0.0\n",
    "\n",
    "    # --- assemble results ---\n",
    "    features = {\n",
    "        \"speech_speed\": float(speech_speed),  # words per second\n",
    "        \"pauses\": float(num_pauses),  # total number of pauses\n",
    "        \"pause_mean\": float(pause_mean),  # average pause duration\n",
    "        \"pause_var\": float(pause_var),  # variance of pause durations\n",
    "        \"vocab_richness\": float(vocab_richness),  # unique words / total words\n",
    "        \"filler_word_rate\": float(filler_word_rate),  # proportion of filler words\n",
    "        \"lexical_diversity\": float(lexical_diversity),  # lexical diversity score\n",
    "        \"avg_sentence_length\": float(avg_sentence_length),  # words per sentence\n",
    "        #\"avg_parse_depth\": float(avg_parse_depth),  # parse tree depth\n",
    "        \"speech_articulation_rate\": float(speech_articulation_rate),  # syllables per second\n",
    "        #\"coherence_score\": float(coherence_score),  # topic coherence variance\n",
    "        \"repetition_rate\": float(repetition_rate),  # repeated word ratio\n",
    "        \"pronoun_noun_ratio\": float(pronoun_noun_ratio),  # pronouns to nouns\n",
    "        \"avg_dependency_length\": float(avg_dependency_length),  # dependency distance\n",
    "        \"tense_ratio_present\": float(tense_ratio_present),  # share of present tense verbs\n",
    "        \"tense_ratio_past\": float(tense_ratio_past),  # share of past tense verbs\n",
    "        \"tense_ratio_future\": float(tense_ratio_future),  # share of future tense verbs\n",
    "        \"semantic_similarity_drift\": float(semantic_similarity_drift),  # difference from baseline\n",
    "    }\n",
    "\n",
    "    return features"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36cb7f90",
   "metadata": {},
   "source": [
    "## Data loading helpersJust simple wrappers to stash daily results."
   ]
  },
  {
   "cell_type": "code",
   "id": "45d88ffc",
   "metadata": {},
   "source": [
    "def load_whisper_outputs(json_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load Whisper transcript segments from a JSON file.\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data.get('segments', [])\n",
    "\n",
    "def update_patient_day_from_json(patient_id: str, day: int, json_path: str, baseline_emb: Optional[np.ndarray] = None):\n",
    "    segments = load_whisper_outputs(json_path)\n",
    "    transcript = ' '.join(seg.get('text', '') for seg in segments)\n",
    "    duration = segments[-1]['end'] if segments else 0\n",
    "    features = compute_features(transcript, duration, segments, baseline_emb)\n",
    "    if patient_id not in history:\n",
    "        history[patient_id] = []\n",
    "    history[patient_id].append({'day': day, **features})\n",
    "    if patient_id not in baseline:\n",
    "        baseline[patient_id] = features\n",
    "\n",
    "    return features\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4ae12449",
   "metadata": {},
   "source": "## plot_trends Super basic plots so we can see what's up.\n"
  },
  {
   "cell_type": "code",
   "id": "60b9f827",
   "metadata": {},
   "source": [
    "def plot_trends(patient_id: str):\n",
    "    df = pd.DataFrame(history.get(patient_id, []))\n",
    "    if df.empty:\n",
    "        print('No history for', patient_id)\n",
    "        return\n",
    "    df.set_index('day', inplace=True)\n",
    "    df.plot(subplots=True, figsize=(12, 18), marker='o')\n",
    "    plt.suptitle(f'Patient {patient_id} - Speech Feature Trends')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c197f4b1",
   "metadata": {},
   "source": [
    "## generate_explainable_report    Prints some diffs vs baseline. monkeyp atch but good enough."
   ]
  },
  {
   "cell_type": "code",
   "id": "0bc58370",
   "metadata": {},
   "source": [
    "def generate_explainable_report(patient_id: str):\n",
    "    if patient_id not in baseline or patient_id not in history:\n",
    "        print('No data for', patient_id)\n",
    "        return\n",
    "    latest = history[patient_id][-1]\n",
    "    base = baseline[patient_id]\n",
    "    print(f'Patient {patient_id} - Day {latest[\"day\"]}')\n",
    "    for k, v in latest.items():\n",
    "        if k == 'day':\n",
    "            continue\n",
    "        change = ((v - base[k]) / base[k] * 100) if base[k] else 0\n",
    "        print(f'{k}: {v:.3f} (change {change:+.1f}%)')\n",
    "    print()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cffe3667",
   "metadata": {},
   "source": [
    "## Mregression all the way. Also threw in a bootstrap helper."
   ]
  },
  {
   "cell_type": "code",
   "id": "cf85a0ac",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_multi_task_model(df: pd.DataFrame):\n",
    "    feature_cols = [c for c in df.columns if c not in ['MMSE', 'MoCA', 'CDR', 'day']]\n",
    "    X = df[feature_cols]\n",
    "    y = df[['MMSE', 'MoCA', 'CDR']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    print(f'Model MSE: {mse:.2f}')\n",
    "    return model\n",
    "\n",
    "def bootstrap_confidence_interval(data: np.ndarray, iterations: int = 1000, alpha: float = 0.05):\n",
    "    samples = []\n",
    "    n = len(data)\n",
    "    for _ in range(iterations):\n",
    "        resample = np.random.choice(data, size=n, replace=True)\n",
    "        samples.append(np.mean(resample))\n",
    "    lower = np.percentile(samples, 100 * (alpha / 2))\n",
    "    upper = np.percentile(samples, 100 * (1 - alpha / 2))\n",
    "    return lower, upper\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da077d8c",
   "metadata": {},
   "source": [
    "## Example run \n",
    "Shows the flow from fake transcripts to predictions."
   ]
  },
  {
   "cell_type": "code",
   "id": "28ff76b1",
   "metadata": {},
   "source": [
    "# Simulated example data\n",
    "patient = 'P001'\n",
    "for day in range(1, 4):\n",
    "    text = 'This is a short example transcript with uh pauses and filler words.'\n",
    "    duration = 30.0\n",
    "    compute = compute_features(text, duration)\n",
    "    if patient not in history:\n",
    "        history[patient] = []\n",
    "        baseline[patient] = compute\n",
    "    history[patient].append({'day': day, **compute})\n",
    "\n",
    "df = pd.DataFrame(history[patient])\n",
    "df['MMSE'] = 30 - df['day'] * 0.5\n",
    "df['MoCA'] = 28 - df['day'] * 0.4\n",
    "df['CDR'] = 0.5 + df['day'] * 0.05\n",
    "\n",
    "model = train_multi_task_model(df)\n",
    "plot_trends(patient)\n",
    "generate_explainable_report(patient)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
